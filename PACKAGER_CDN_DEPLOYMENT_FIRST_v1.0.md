# FPT PLAY - PACKAGER + CDN DEPLOYMENT (PRIORITY ORDER)
**Status: Production Implementation Plan for Packager-First Strategy**

---

## ğŸ¯ Táº I SAO PACKAGER + CDN TRÆ¯á»šC?

```
FPT Play Revenue Flow:
  
  Satellite/ISP Feed
       â†“
  Encoder (H.264/HEVC)
       â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  **PACKAGER** â† HE FLOW STARTS   â”‚  â† OTT Streaming (Revenue Core)
  â”‚  (HLS/DASH ABR)                 â”‚    - Äi qua packager TRÆ¯á»šC
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    - Rá»“i Ä‘áº©y sang CDN
       â†“
  CDN Origin/Cache
       â†“
  Edge POPs (HN, HCM, Regions)
       â†“
  Clients (Web, App, STB)
  
ğŸ’° Revenue Impact:
  - Packager = Gateway cho OTT â†’ pháº£i reliable
  - Náº¿u packager down â†’ OTT revenue = 0
  - Encoder down â†’ IPTV affected, nhÆ°ng OTT (HE) váº«n cÃ³ cache

ğŸ‘‰ CHIáº¾N LÆ¯á»¢C: GiÃ¡m sÃ¡t Packager + CDN TRÆ¯á»šC
   - Äáº£m báº£o dÃ²ng tiá»n OTT á»•n Ä‘á»‹nh
   - Sau Ä‘Ã³ má»›i lo IPTV (L1 Headend)
```

---

## ğŸ“Š DEPLOYMENT SEQUENCE (WEEK-BY-WEEK)

### WEEK 1-2: PACKAGER LAYER (L2)
```
FOCUS: HLS/DASH ABR generation + segment validation

TASK 1: Deploy Inspector LIVE #2 + #2B (Redundancy)
  â”œâ”€ LIVE-PACKAGER-01 (Primary)     â†’ 10.10.20.21
  â”œâ”€ LIVE-PACKAGER-02 (Backup)      â†’ 10.10.20.22
  â””â”€ Both monitored together (HA config)

TASK 2: Configure Packager Input
  â”œâ”€ Source: Encoder TS (MPEG-TS from L1)
  â”‚   Input multicast: 239.1.0.0/16, 239.2.0.0/16
  â”‚   OR HTTP stream from encoder (fallback)
  â”œâ”€ Connect 50 channels initially:
  â”‚   â”œâ”€ Top 20 HD live
  â”‚   â”œâ”€ Top 10 4K HDR live
  â”‚   â”œâ”€ Top 20 most watched VOD
  â”‚   â””â”€ Ad-supported channels
  â””â”€ Verify TS reception on probe

TASK 3: ABR Ladder Monitoring
  â”œâ”€ Expected manifest structure:
  â”‚   /live/CH_001/master.m3u8 (Root playlist)
  â”‚   â”œâ”€ /CH_001_4K_15M.m3u8     (4K: 15 Mbps)
  â”‚   â”œâ”€ /CH_001_HD_5M.m3u8      (HD: 5 Mbps)
  â”‚   â”œâ”€ /CH_001_SD_2M.m3u8      (SD: 2 Mbps)
  â”‚   â””â”€ /CH_001_LD_500K.m3u8    (Low: 500 Kbps)
  â”œâ”€ Validate ladder completeness
  â”œâ”€ Check bitrate spacing (should match spec)
  â””â”€ Alert if missing rungs

TASK 4: Segment Continuity Check
  â”œâ”€ Download + verify:
  â”‚   â”œâ”€ Segment duration (target: 6s Â± 10%)
  â”‚   â”œâ”€ Sequence number continuity (no gaps)
  â”‚   â”œâ”€ Media sequence increment
  â”‚   â”œâ”€ Target duration match
  â”‚   â””â”€ Byte range accuracy (fMP4)
  â”œâ”€ Monitor for:
  â”‚   â”œâ”€ DISCONTINUITY tags without warning
  â”‚   â”œâ”€ Missing segments (HTTP 404)
  â”‚   â”œâ”€ Segment duration variance
  â”‚   â””â”€ Timeline drift
  â””â”€ Alert on abnormalities

TASK 5: TS-to-ABR Mapping Verification
  â”œâ”€ For each channel:
  â”‚   â”œâ”€ Compare TS timestamp â†’ first segment timestamp
  â”‚   â”œâ”€ Verify continuity across renditions
  â”‚   â”œâ”€ Check audio/subtitle presence in each rung
  â”‚   â””â”€ Validate EBP (Entry Point Boundary) if used
  â”œâ”€ Ensure MOS/QoS metrics map correctly
  â””â”€ Alert if mapping degradation detected

COMPLETION: 
  âœ“ 50 channels monitored end-to-end (TS â†’ ABR)
  âœ“ Packager health dashboard live
  âœ“ Segment continuity alerts enabled
```

---

### WEEK 3: CDN CORE LAYER (L3)
```
FOCUS: Origin + MidCache health monitoring

TASK 1: Deploy Inspector LIVE #3 (CDN Core Probe)
  â”œâ”€ LIVE-CDNCORE-01: 10.10.30.21
  â”œâ”€ Location: Cáº¡nh Origin cluster hoáº·c Coping MidCache
  â”œâ”€ Input: Sample 20-30 top channels via HTTP
  â”‚   â”œâ”€ Sample from Origin: http://origin.cdn.internal/live/CH_001/
  â”‚   â”œâ”€ OR from MidCache: http://midcache.cdn.internal/live/CH_001/
  â”‚   â””â”€ Both if possible (in series)
  â””â”€ Verify connectivity to CDN infrastructure

TASK 2: Origin Health Monitoring
  â”œâ”€ Measure per-segment fetch time:
  â”‚   â”œâ”€ HTTP request latency (should be < 500ms)
  â”‚   â”œâ”€ First byte latency (should be < 200ms)
  â”‚   â”œâ”€ Last byte to complete (overall < 2s)
  â”œâ”€ Count HTTP status codes:
  â”‚   â”œâ”€ 200 OK (success)
  â”‚   â”œâ”€ 206 Partial Content (range requests - OK)
  â”‚   â”œâ”€ 304 Not Modified (cache check - OK)
  â”‚   â”œâ”€ 4xx Client errors (manifest not found - ALERT)
  â”‚   â””â”€ 5xx Server errors (origin down - CRITICAL)
  â”œâ”€ Monitor origin load:
  â”‚   â”œâ”€ CPU/Memory/Network utilization
  â”‚   â”œâ”€ Concurrent connections
  â”‚   â””â”€ Request queue depth
  â””â”€ Alert thresholds:
      - HTTP 5xx > 1% â†’ CRITICAL
      - Latency p95 > 1s â†’ MAJOR
      - Origin CPU > 80% â†’ WARNING

TASK 3: MidCache Performance
  â”œâ”€ Monitor cache hit ratio:
  â”‚   â”œâ”€ X-Cache header: HIT vs MISS
  â”‚   â”œâ”€ Target: > 90% HIT rate
  â”‚   â”œâ”€ If < 85%: investigate cache eviction
  â”‚   â””â”€ Alert if drop below 80%
  â”œâ”€ Cache layer statistics:
  â”‚   â”œâ”€ Bytes served from cache (total bandwidth)
  â”‚   â”œâ”€ Bytes requested from origin (cache miss cost)
  â”‚   â”œâ”€ Cache eviction rate
  â”‚   â””â”€ Stale content serving (if any)
  â”œâ”€ Per-channel cache performance:
  â”‚   â”œâ”€ Top 10 channels: Expect > 95% HIT
  â”‚   â”œâ”€ Mid tier: Expect > 85% HIT
  â”‚   â””â”€ Long tail: Expect > 70% HIT
  â””â”€ Compare expected vs actual hit rates

TASK 4: Segment Availability from CDN
  â”œâ”€ Random sampling:
  â”‚   â”œâ”€ Pick 5 random channels (every hour)
  â”‚   â”œâ”€ Request latest 3 segments
  â”‚   â”œâ”€ Measure response time
  â”‚   â”œâ”€ Verify segment integrity (byte count, duration)
  â”‚   â””â”€ Check if in cache or fetched from origin
  â”œâ”€ Error tracking:
  â”‚   â”œâ”€ Count 404s (segment missing)
  â”‚   â”œâ”€ Count 503s (service unavailable)
  â”‚   â”œâ”€ Count timeouts
  â”‚   â””â”€ Alert if error rate > 0.5%
  â””â”€ Manifest consistency:
      - Master playlist reflects actual segments
      - Bandwidth declarations accurate
      - Program date/time integrity

TASK 5: CDN Edge Readiness
  â”œâ”€ Pre-validate edge POPs before full traffic:
  â”‚   â”œâ”€ Can edge reach origin? (connectivity check)
  â”‚   â”œâ”€ Edge disk space OK? (> 100GB free recommended)
  â”‚   â”œâ”€ Edge CPU/memory baseline established
  â”‚   â””â”€ Edge network capacity available
  â”œâ”€ Pre-warm important channels to edge:
  â”‚   â”œâ”€ Push top 20 channels to Edge HN
  â”‚   â”œâ”€ Push top 20 channels to Edge HCM
  â”‚   â”œâ”€ Verify arrival at edge
  â”‚   â””â”€ Measure edge fetch latency (should be < 100ms local)
  â””â”€ Test failover: Origin down â†’ Edge should serve from cache

COMPLETION:
  âœ“ CDN core monitoring live (Origin + MidCache)
  âœ“ Cache hit ratio visible and tracked
  âœ“ Segment availability from CDN validated
  âœ“ Edge POPs pre-warmed and ready
```

---

### WEEK 4: INTEGRATION + PRODUCTION CUTOVER
```
FOCUS: Packager + CDN working together, ready for live OTT traffic

TASK 1: End-to-End Flow Validation (TS â†’ Packager â†’ CDN â†’ Client)
  â”œâ”€ Encoder generates TS
  â”œâ”€ Probe L2 monitors TS quality
  â”œâ”€ Packager generates HLS/DASH
  â”œâ”€ Probe L2 validates ABR quality
  â”œâ”€ CDN picks up manifest + segments
  â”œâ”€ Probe L3 monitors CDN health
  â”œâ”€ Simulate client download (HLS.js / dash.js)
  â”œâ”€ Verify playback quality (MOS)
  â”œâ”€ Compare MOS: TS vs ABR vs Edge
  â””â”€ Alert if degradation detected at any layer

TASK 2: Redundancy Configuration
  â”œâ”€ Packager failover:
  â”‚   â”œâ”€ Primary LIVE-PACKAGER-01 â†” Backup LIVE-PACKAGER-02
  â”‚   â”œâ”€ If one goes down, alerts on both
  â”‚   â”œâ”€ Test failover manually
  â”‚   â””â”€ Verify backup takes over seamlessly
  â”œâ”€ CDN failover:
  â”‚   â”œâ”€ Origin primary + backup
  â”‚   â”œâ”€ MidCache distributed for redundancy
  â”‚   â”œâ”€ Test origin failure â†’ cache continues serving
  â”‚   â””â”€ Verify no service interruption
  â””â”€ Monitoring continues through failover

TASK 3: Threshold Tuning (Packager + CDN)
  â”œâ”€ ABR Ladder:
  â”‚   â”œâ”€ Min rung bitrate: _____ kbps (target: < 1 Mbps)
  â”‚   â”œâ”€ Max rung bitrate: _____ Mbps (target: 15-25 for 4K)
  â”‚   â”œâ”€ Typical 4K ladder: 500K, 2M, 5M, 10M, 15M
  â”‚   â””â”€ Verify ladder matches hardware capabilities
  â”œâ”€ Segment Duration:
  â”‚   â”œâ”€ Standard: 6 seconds
  â”‚   â”œâ”€ Tolerance: Â± 10% (5.4 - 6.6 sec)
  â”‚   â”œâ”€ Playlist size: 3 segments minimum
  â”‚   â””â”€ Monitor duration variance
  â”œâ”€ Cache Thresholds:
  â”‚   â”œâ”€ Hit ratio target: > 90%
  â”‚   â”œâ”€ Origin latency target: < 500ms
  â”‚   â”œâ”€ Edge latency target: < 100ms
  â”‚   â””â”€ Total download time: < 2s per segment
  â””â”€ Quality Thresholds:
      - ABR MOS: > 4.0 (same as TS)
      - Video macroblocking: < 10% (no increase from TS)
      - Frame drop rate: 0%
      - Stall ratio: < 1%

TASK 4: Alert Routing + Escalation
  â”œâ”€ CRITICAL Alerts (Packager):
  â”‚   â”œâ”€ Packager down
  â”‚   â”œâ”€ All manifests missing (404)
  â”‚   â”œâ”€ All segments missing (404)
  â”‚   â””â”€ â†’ Escalate: Packager Team NOW
  â”œâ”€ CRITICAL Alerts (CDN):
  â”‚   â”œâ”€ Origin down (5xx errors > 10%)
  â”‚   â”œâ”€ Cache hit < 50% (suspected failure)
  â”‚   â”œâ”€ HTTP error rate > 1%
  â”‚   â””â”€ â†’ Escalate: CDN Team NOW
  â”œâ”€ MAJOR Alerts:
  â”‚   â”œâ”€ MOS degradation (4.0 â†’ 2.5)
  â”‚   â”œâ”€ Segment latency p95 > 1s
  â”‚   â”œâ”€ Cache hit ratio 85-90% (trend warning)
  â”‚   â””â”€ â†’ Monitor, escalate if not resolved in 5 min
  â””â”€ Route to SNMP â†’ Zabbix â†’ ON-CALL escalation

TASK 5: Production Cut-Over Readiness
  â”œâ”€ Checklist:
  â”‚   â˜ Packager probe: Both online, receiving TS, outputting ABR
  â”‚   â˜ CDN core probe: Online, monitoring origin + midcache
  â”‚   â˜ Alert rules: All configured (CRITICAL, MAJOR, MINOR)
  â”‚   â˜ Escalation lists: Teams have contact info
  â”‚   â˜ Thresholds: Baselined against 24h data
  â”‚   â˜ Runbooks: Written and team trained
  â”‚   â˜ Dashboard: Packager + CDN live in Grafana/iVMS
  â”‚   â˜ Fail-over tested: Both packager and CDN
  â”‚   â˜ Documentation: Channel mapping, IP config, contact list
  â”‚   â˜ Customer notification: Ops team aware of go-live
  â”œâ”€ Sign-off required:
  â”‚   â”œâ”€ Packager Team Lead
  â”‚   â”œâ”€ CDN Team Lead
  â”‚   â”œâ”€ NOC Manager
  â”‚   â””â”€ Platform Lead
  â””â”€ Green light â†’ Go live

COMPLETION:
  âœ“ Packager + CDN monitoring 100% operational
  âœ“ 50 channels flowing through OTT pipeline
  âœ“ End-to-end MOS tracked and alerted
  âœ“ Redundancy validated
  âœ“ Teams trained and ready for 24/7 ops
```

---

## ğŸ“‹ DETAILED TASKS BY ROLE (WHO DOES WHAT)

### ğŸ”§ PACKAGER TEAM

#### Task P1: Packager Infrastructure Setup
```
WHEN: Week 1, Day 1-2
WHO: Packager Engineer + DevOps
TIME: 4 hours

STEPS:
1. Verify Packager Hardware
   $ ssh packager-01.internal
   $ # Check available resources
   $ lscpu
   $ free -h
   $ df -h
   
   Requirements:
   - CPU: 16+ cores available
   - RAM: 32GB+ available
   - Disk: > 1TB SSD for cache
   - Network: 10Gbps NIC dedicated to contribution

2. Check Packager Configuration
   $ # If using Elemental, Unified, or Wowza
   $ service elemental status
   $ ps aux | grep -i packager
   $ # Verify running and healthy

3. Configure Input Feeds
   $ # TS input from encoder multicast
   $ ip route add 239.0.0.0/8 dev eth0
   $ ip maddr add 239.1.1.1 dev eth0
   $ # Test reception
   $ tcpdump -i eth0 "dst 239.1.1.1" -c 100

4. Configure Output Manifests
   $ # Packager should output:
   $ ls -la /var/www/live/
   /var/www/live/CH_001/master.m3u8
   /var/www/live/CH_001/CH_001_4K_15M.m3u8
   /var/www/live/CH_001/CH_001_HD_5M.m3u8
   /var/www/live/CH_001/segments/
   
   $ # Test manifest fetch
   $ curl http://localhost/live/CH_001/master.m3u8

5. Verify Output to CDN
   $ # Check if segments being written to origin mount
   $ ls /mnt/origin/live/CH_001/
   $ # Should see segment-0000.ts, segment-0001.ts, etc.
   
DELIVERABLE: Packager ready to accept TS + produce ABR
```

#### Task P2: Create 50 Channel Profiles
```
WHEN: Week 1, Day 2-3
WHO: Packager Engineer + Content Team
TIME: 6 hours

STEPS:
1. List 50 Priority Channels
   Priority 1 (20): Top rated, news, sports, movies
   Priority 2 (20): Secondary popular, regional, branded
   Priority 3 (10): Ad-supported, sponsored

2. For Each Channel, Define:
   a. Channel ID / Name
      Example: CH_TV_HD_001 / "FPT Channel 1"
   
   b. Input Source
      Multicast: 239.1.x.y:1234 (prog_num=101)
      OR HTTP: http://encoder/stream/ch001.ts
   
   c. ABR Ladder (for 4K channels)
      â”œâ”€ 4K HDR: 15 Mbps, 3840x2160, HEVC-10, HLG/HDR10
      â”œâ”€ HD: 5 Mbps, 1920x1080, HEVC or H.264
      â”œâ”€ SD: 2 Mbps, 1280x720
      â””â”€ LD: 500 Kbps, 640x360
   
   d. ABR Ladder (for HD channels)
      â”œâ”€ HD: 5 Mbps, 1920x1080
      â”œâ”€ SD: 2 Mbps, 1280x720
      â””â”€ LD: 500 Kbps, 640x360
   
   e. Audio Tracks
      Primary: AC-3 5.1, -24 LUFS
      Secondary: AAC Stereo, -24 LUFS (optional)
   
   f. Subtitles
      Format: CEA-608 in video OR separate SRT
   
   g. Ad Insertion
      SCTE-35: Enabled (segment count for ad breaks)
      Ad duration: 30s (typical)

3. Create Packager Profile (XML/Config Example)
   
   <Channel>
     <ChannelID>CH_TV_HD_001</ChannelID>
     <Name>FPT Channel 1</Name>
     <Input>
       <Type>MPEG-TS</Type>
       <Multicast>239.1.1.1:1234</Multicast>
       <ProgramNumber>101</ProgramNumber>
     </Input>
     <Output>
       <Playlist>/live/CH_TV_HD_001/master.m3u8</Playlist>
       <SegmentPath>/live/CH_TV_HD_001/segments/</SegmentPath>
       <SegmentDuration>6</SegmentDuration>
       <PlaylistType>EVENT</PlaylistType>  <!-- or VOD for recorded -->
     </Output>
     <Renditions>
       <Rendition bitrate="5000" resolution="1920x1080" codec="avc"/>
       <Rendition bitrate="2000" resolution="1280x720" codec="avc"/>
       <Rendition bitrate="500" resolution="640x360" codec="avc"/>
     </Renditions>
     <SCTE35>
       <Enabled>true</Enabled>
       <SegmentCountForAdBreak>1</SegmentCountForAdBreak>
     </SCTE35>
   </Channel>

4. Import/Configure in Packager
   $ # Elemental Manager:
   $ # UI: Create channel from template â†’ fill in CH_001 values
   $ # Or via API:
   curl -X POST https://packager/api/channels \
     -d @channel_profile.json
   
   $ # Unified Packager:
   $ vim /etc/packager/channels/CH_001.conf
   $ systemctl reload packager

5. Verification per Channel
   âœ“ Input TS received (packets counted)
   âœ“ Manifest generated
   âœ“ All renditions present in playlist
   âœ“ First segment available
   âœ“ Can download and play segment

DELIVERABLE: 50 channels configured, ingesting, outputting ABR
```

#### Task P3: Segment Quality Validation
```
WHEN: Week 1-2, continuous
WHO: Packager QA Engineer
TIME: 8 hours (setup), then 2h/day monitoring

STEPS:
1. Automated Segment Checker (Python Script)
   
   #!/usr/bin/env python3
   import requests
   import m3u8
   from datetime import datetime
   
   def check_channel_segments(channel_id, probe_url):
       """Download and validate segments"""
       
       # 1. Get master playlist
       resp = requests.get(f"http://packager/live/{channel_id}/master.m3u8")
       master = m3u8.loads(resp.text)
       
       # 2. For each rendition
       for variant in master.variants:
           rung_url = f"http://packager{variant.uri}"
           resp = requests.get(rung_url)
           playlist = m3u8.loads(resp.text)
           
           # 3. Validate playlist structure
           assert playlist.target_duration == 6, "Wrong segment duration"
           assert len(playlist.segments) >= 3, "Playlist too short"
           
           # 4. Check segment integrity
           for i, seg in enumerate(playlist.segments):
               seg_url = f"{rung_url.rsplit('/', 1)[0]}/{seg.uri}"
               
               # Measure download
               start = datetime.now()
               seg_resp = requests.get(seg_url)
               elapsed = (datetime.now() - start).total_seconds()
               
               # Validation
               assert seg_resp.status_code == 200, f"Segment {i} failed"
               assert len(seg_resp.content) > 10000, "Segment too small"
               assert elapsed < 2, f"Segment took {elapsed}s to download"
               
               # Check duration
               assert abs(seg.duration - 6.0) < 0.5, f"Duration off"
       
       print(f"âœ“ {channel_id}: All segments valid")

2. Run Validation on All 50 Channels
   for ch_id in CH_001 to CH_050:
     check_channel_segments(ch_id, "http://packager")
   
   Time: ~5 min per channel if sequential
        or 30 sec if parallel on 10 workers

3. Create Segment Metrics Dashboard
   Track per-channel:
   - Segment arrival rate (segments/min)
   - Average download time (ms)
   - Download time p95 (ms)
   - Error rate (failed segments %)
   - Bandwidth per rung (Mbps)
   
   $ # Example query (InfluxDB)
   SELECT MEAN("download_time_ms") FROM segment_metrics
   WHERE channel_id = 'CH_001' AND time > now() - 1h
   GROUP BY rendition

DELIVERABLE: Segment quality dashboard live, validates 50 channels
```

#### Task P4: Packager Redundancy Setup
```
WHEN: Week 2, Day 3
WHO: Packager Engineer + DevOps
TIME: 4 hours

OBJECTIVE: If LIVE-PACKAGER-01 goes down, traffic switches to LIVE-PACKAGER-02

STEPS:
1. Configure Active-Active Packager Pair
   
   Packager-01 (Primary):
   â”œâ”€ IP: 10.10.20.21
   â”œâ”€ Receives TS: 239.1.x.x (50 channels)
   â”œâ”€ Outputs: /mnt/origin/live/CH_*/
   â””â”€ Publishes: http://packager-01/live/
   
   Packager-02 (Standby / Active):
   â”œâ”€ IP: 10.10.20.22
   â”œâ”€ Receives TS: Same multicast groups (IGMP join both)
   â”œâ”€ Outputs: /mnt/origin/live/CH_*/  (same NFS mount)
   â””â”€ Publishes: http://packager-02/live/
   
   Load Balancer (VIP):
   â”œâ”€ Virtual IP: 10.10.20.10 (http://packager-vip/live/)
   â”œâ”€ Routes to: packager-01 (weight 100)
   â”œâ”€ Routes to: packager-02 (weight 0 - standby)
   â””â”€ Health check: GET /health â†’ HTTP 200

2. Setup Shared Storage
   $ # NFS mount on both packagers
   $ mount -t nfs nfs-server:/export/origin /mnt/origin
   $ # Verify mount
   $ df -h /mnt/origin
   
   Advantage: If packager-01 crashes, packager-02 continues
   serving same segments from shared cache

3. Configure Health Checks
   $ # On load balancer:
   $ # Check every 5 sec
   $ # If packager-01 returns 5xx or timeout for 15 sec
   $ # â†’ Failover to packager-02
   
   Health check URL: http://packager-01:8080/api/health
   
   Response:
   {
     "status": "healthy",
     "channels_active": 50,
     "output_bitrate_mbps": 250,
     "cpu_usage_percent": 35,
     "disk_usage_percent": 60
   }

4. Test Failover
   Step 1: Shut down packager-01
   $ service packager stop
   
   Step 2: Monitor alerts
   - Probe should alert "Packager-01 down"
   - Load balancer should route to packager-02
   
   Step 3: Verify playback continues
   $ # Download segment from VIP
   $ curl http://packager-vip/live/CH_001/segments/segment-100.ts
   $ # Should succeed (served from packager-02)
   
   Step 4: Restart packager-01
   $ service packager start
   $ # Load balancer re-adds it back
   
   Step 5: Verify no service interruption
   - Segments continued flowing
   - Only brief interruption (< 5 sec) if any

DELIVERABLE: Packager redundancy validated, failover tested
```

---

### ğŸŒ CDN TEAM

#### Task C1: CDN Infrastructure Audit
```
WHEN: Week 2, Day 1-2
WHO: CDN Engineer + Infrastructure Team
TIME: 6 hours

OBJECTIVE: Understand CDN topology, document current performance baseline

STEPS:
1. Map CDN Architecture
   
   ORIGIN CLUSTER:
   â”œâ”€ Origin-01: 10.40.1.10 (Primary)
   â”œâ”€ Origin-02: 10.40.1.11 (Backup)
   â””â”€ Shared Storage: /var/www/origin (NFS)
   
   MIDCACHE LAYER:
   â”œâ”€ MidCache-HN: 10.40.10.10 (Hanoi region)
   â”œâ”€ MidCache-HCM: 10.40.20.10 (Ho Chi Minh region)
   â””â”€ MidCache-DN: 10.40.30.10 (Da Nang region)
   
   EDGE POPs:
   â”œâ”€ Edge-HN-01: 10.20.10.21 (Hanoi)
   â”œâ”€ Edge-HN-02: 10.20.10.22 (Hanoi backup)
   â”œâ”€ Edge-HCM-01: 10.20.20.21 (Ho Chi Minh)
   â”œâ”€ Edge-HCM-02: 10.20.20.22 (Ho Chi Minh backup)
   â””â”€ Edge-DN-01: 10.20.30.21 (Da Nang)

2. Document Network Paths
   Packager â†’ Origin:
   $ traceroute origin-01.cdn.internal
   
   Origin â†’ MidCache:
   $ traceroute midcache-hn.cdn.internal
   
   MidCache â†’ Edge:
   $ traceroute edge-hn-01.cdn.internal

3. Check Origin Storage
   $ ssh origin-01.cdn.internal
   $ df -h /var/www/origin
   
   Available space: ________ GB
   Used space: ________ GB
   Recommend: Keep > 20% free for operational headroom

4. Verify Cache Setup
   $ ssh midcache-hn.cdn.internal
   $ # Check cache directory
   $ du -sh /var/cache/cdn
   
   Cache size: ________ GB
   Recommend: 500GB+ SSD for HN midcache

5. Document Baseline Metrics
   
   Per-Origin:
   â”œâ”€ Current throughput: ______ Mbps
   â”œâ”€ Current connections: ______
   â”œâ”€ Current CPU: ______%
   â”œâ”€ Current memory: ______%
   â””â”€ Disk I/O: ______ IOPS
   
   Per-MidCache:
   â”œâ”€ Cache hit ratio: ______%
   â”œâ”€ Bytes in cache: ______ GB
   â””â”€ Eviction rate: ______/hour
   
   Per-Edge:
   â”œâ”€ Throughput to clients: ______ Mbps
   â””â”€ Latency to clients: ______ ms

DELIVERABLE: CDN topology documented, baseline established
```

#### Task C2: Origin Monitoring Configuration
```
WHEN: Week 2, Day 2-3
WHO: CDN Engineer + Monitoring Admin
TIME: 5 hours

OBJECTIVE: Setup Inspector LIVE #3 to monitor Origin health

STEPS:
1. Deploy Inspector LIVE #3
   $ # On VM LIVE-CDNCORE-01
   $ ssh live-cdncore-01.monitor.local
   $ curl https://packager/live/CH_001/master.m3u8 -I
   
   Configure Inspector:
   â”œâ”€ Primary source: Origin-01
   â”œâ”€ Channels to monitor: CH_001-020 (top 20)
   â”œâ”€ Secondary source: MidCache-HN (for cache hit verification)
   â””â”€ Polling interval: Every 30 seconds

2. Create Origin Monitoring Template
   
   ```yaml
   CDN_Origin_Template:
     input_type: HTTP_ABR
     sources:
       - url: http://origin-01/live/
       - url: http://midcache-hn/live/
     
     metrics:
       http_latency:
         target: < 500ms (p95)
         alert_threshold: > 1000ms
       
       http_errors:
         target: < 0.1%
         alert_threshold: > 1%
       
       segment_availability:
         target: 100%
         alert_threshold: < 99%
       
       cache_hit_ratio:
         target: > 90%
         alert_threshold: < 80%
     
     sampling:
       interval: 30s
       per_channel: 1 sample per interval
       per_rendition: Sample each quality rung
   ```

3. Configure HTTP-level Monitoring
   
   For each segment downloaded:
   â”œâ”€ Measure HTTP response time (ms)
   â”œâ”€ Check HTTP status code (200, 206, 404, 5xx)
   â”œâ”€ Verify Content-Length matches downloaded bytes
   â”œâ”€ Check X-Cache header:
   â”‚  â”œâ”€ X-Cache: HIT (served from cache) âœ“
   â”‚  â”œâ”€ X-Cache: MISS (fetched from origin) âœ“
   â”‚  â””â”€ X-Cache: None (bypass cache) âš 
   â””â”€ Verify Last-Modified / ETag consistency

4. Setup Per-Origin Error Tracking
   
   Origin-01 Status:
   â”œâ”€ HTTP 200: ____/hour (should be 90-100%)
   â”œâ”€ HTTP 206: ____/hour (partial content, OK for range requests)
   â”œâ”€ HTTP 304: ____/hour (not modified, OK)
   â”œâ”€ HTTP 4xx: ____/hour (should be < 1%)
   â”œâ”€ HTTP 5xx: ____/hour (should be 0, ALERT if > 0)
   â””â”€ Timeout: ____/hour (should be 0)

5. Configure Cache Hit Analysis
   
   $ # Query CDN logs
   $ tail -10000 /var/log/nginx/access.log | \
     awk '{print $NF}' | sort | uniq -c
   
   Expected output:
   HIT_fresh: 91000
   MISS: 8000
   EXPIRED: 1000
   
   Analysis:
   â”œâ”€ Hit ratio: 91000 / 100000 = 91% âœ“
   â”œâ”€ Investigate if < 85%
   â””â”€ Check for cache eviction issues

6. Create Origin Health Dashboard
   
   Grafana Panel 1: HTTP Latency (p50, p95, p99)
   Grafana Panel 2: HTTP Error Rate
   Grafana Panel 3: Cache Hit Ratio
   Grafana Panel 4: Segment Availability %
   Grafana Panel 5: Origin CPU/Memory
   
   Alert Rules:
   â”œâ”€ If p95 latency > 1s â†’ MAJOR
   â”œâ”€ If error rate > 1% â†’ CRITICAL
   â”œâ”€ If cache hit < 85% â†’ MAJOR
   â””â”€ If availability < 99.9% â†’ CRITICAL

DELIVERABLE: Origin monitoring live, latency/errors/cache visible
```

#### Task C3: MidCache Performance Tuning
```
WHEN: Week 2, Day 3-4
WHO: CDN Cache Engineer
TIME: 6 hours

OBJECTIVE: Optimize cache hit ratio to > 90%

STEPS:
1. Analyze Cache Hit Ratio Baseline
   
   Current state (before tuning):
   $ ssh midcache-hn.cdn.internal
   $ # Extract cache hit stats
   $ grep "X-Cache: HIT" /var/log/nginx/access.log | wc -l
   $ grep "X-Cache: MISS" /var/log/nginx/access.log | wc -l
   
   Example: 10,000 HIT + 1,500 MISS = 87% hit ratio
   Target: > 90%
   Gap: +3 percentage points needed

2. Identify Miss Reasons
   
   Type 1: Segment Not in Cache Yet
   â””â”€ First request for segment â†’ MISS (expected)
   
   Type 2: Segment Expired (TTL exceeded)
   â””â”€ Increase TTL (Cache-Control header)
   
   Type 3: Cache Eviction (LRU - Least Recently Used)
   â””â”€ Cache full, popular segments get evicted
   â””â”€ Solution: Increase cache size or reduce TTL on low-popularity content
   
   Type 4: Cache Bypass (Set-Cookie or other rules)
   â””â”€ Check if origin sending no-cache headers
   â””â”€ Fix: Configure conditional caching in CDN rules

3. Tuning Actions
   
   Action A: Increase Cache TTL for Segments
   
   Origin currently sends:
   Cache-Control: max-age=60 (1 minute)
   
   Update to:
   Cache-Control: max-age=3600 (1 hour)
   
   $ # Modify origin config
   $ vim /etc/nginx/nginx.conf
   
   upstream origin {
     add_header Cache-Control "max-age=3600, public";
   }
   
   $ systemctl reload nginx
   
   Action B: Increase MidCache Disk Size
   
   Current: 256 GB
   Recommend: 500 GB
   
   If diskless issue:
   $ # Add new SSD
   $ # Mount at /var/cache/cdn
   $ # Restart cache daemon
   
   Action C: Tune Cache Eviction Policy
   
   $ # Reduce minimum object size to cache
   $ # (cache even small segments, but with short TTL)
   vim /etc/cache/config.conf
   
   min_cache_size = 1MB (was 10MB)
   max_cache_size = 450GB (was 200GB)
   eviction_policy = LRU_WITH_PRIORITY
   priority_channels = CH_001,CH_002,...  (top 50)

4. Monitor Cache Behavior After Tuning
   
   $ # Compare before/after
   Time Period: 1 hour baseline
   
   Before Tuning:
   â”œâ”€ Hit ratio: 87%
   â”œâ”€ Avg latency: 250ms
   â”œâ”€ Bytes from origin: 150 GB/hour
   â””â”€ Miss rate: 13%
   
   After Tuning:
   â”œâ”€ Hit ratio: 92% (target achieved âœ“)
   â”œâ”€ Avg latency: 120ms (faster!)
   â”œâ”€ Bytes from origin: 40 GB/hour (less origin load!)
   â””â”€ Miss rate: 8%

5. Per-Channel Cache Optimization
   
   Top channels (High Priority):
   â”œâ”€ CH_001-020: Target 95%+ hit ratio
   â”œâ”€ TTL: 1 hour minimum
   â””â”€ Pre-populate to edge on cache miss
   
   Mid-tier channels (Medium Priority):
   â”œâ”€ CH_021-050: Target 85%+ hit ratio
   â”œâ”€ TTL: 30 minutes
   â””â”€ Standard LRU eviction
   
   Long-tail channels (Low Priority):
   â”œâ”€ CH_051+: Acceptable 70%+ hit ratio
   â”œâ”€ TTL: 10 minutes
   â””â”€ More aggressive eviction

DELIVERABLE: Cache hit ratio > 90% verified
```

#### Task C4: Edge POP Readiness
```
WHEN: Week 3, Day 1-2
WHO: CDN Edge Engineer + Network Team
TIME: 4 hours

OBJECTIVE: Prepare edge POPs for content delivery

STEPS:
1. Verify Edge â†’ MidCache Connectivity
   
   Test from Edge HN:
   $ ssh edge-hn-01.cdn.internal
   $ ping midcache-hn.cdn.internal
   Latency: _______ ms (should be < 50ms, same datacenter)
   
   Test from Edge HCM:
   $ ssh edge-hcm-01.cdn.internal
   $ ping midcache-hcm.cdn.internal
   Latency: _______ ms (should be < 50ms)
   
   If latency high:
   â””â”€ Investigate routing, network path
   â””â”€ Check for congestion on inter-DC links

2. Pre-Warm Important Channels to Edge
   
   Goal: Reduce first-request latency
   
   $ # List top 20 channels
   TOP_CHANNELS=(CH_001 CH_002 CH_003 ... CH_020)
   
   for ch in "${TOP_CHANNELS[@]}"; do
     for rung in 4K_15M HD_5M SD_2M; do
       # Download 3 segments to populate cache
       for i in 1 2 3; do
         curl -I "http://midcache-hn/live/$ch/${ch}_${rung}/segment-$(($i + 100)).ts" > /dev/null
       done
     done
   done
   
   Verification:
   $ # Check edge cache
   $ du -sh /var/cache/cdn/live/
   Expected: 20-50 GB pre-warmed

3. Configure Edge â†’ Client Delivery
   
   Edge now ready to serve clients:
   â”œâ”€ Client requests: CH_001/master.m3u8
   â”œâ”€ Edge serves from cache: X-Cache: HIT
   â”œâ”€ Latency to client: < 100ms (ideal)
   â””â”€ Bandwidth: Full client capacity
   
   Measure:
   $ # Simulate client download
   $ curl -I http://edge-hn-01/live/CH_001/segments/segment-100.ts
   
   Response:
   HTTP/1.1 200 OK
   X-Cache: HIT from edge-hn-01
   Content-Length: 500000
   Server: nginx/1.20.0

4. Configure Failover Path
   
   Normal flow:
   Edge HN (full cache) â†’ Client
   
   Edge cache empty:
   Edge HN (miss) â†’ MidCache HN (maybe miss) â†’ Origin
   
   If origin slow:
   Edge can fetch from alternate origin:
   â”œâ”€ Intelligent routing based on RTT
   â”œâ”€ Avoid timeouts by using backup
   â””â”€ Configure in CDN rules

5. Test Edge Failover
   
   Step 1: Simulate edge cache full
   $ # Trigger cache eviction of a channel
   
   Step 2: Request segment â†’ cache miss
   $ curl http://edge-hn-01/live/CH_001/segment-200.ts
   X-Cache: MISS (not in edge cache)
   
   Step 3: Edge fetches from midcache
   X-Cache: HIT from midcache (in mid cache)
   
   Step 4: Midcache miss â†’ fetches from origin
   X-Cache: MISS from midcache (needs origin)
   
   Latency should be:
   â”œâ”€ Edge HIT: < 50ms
   â”œâ”€ MidCache HIT: < 200ms
   â”œâ”€ Origin fetch: < 500ms
   â””â”€ Total: All within acceptable range

DELIVERABLE: Edge POPs ready, pre-warmed, failover tested
```

---

## ğŸ“Š MONITORING SCHEMA FOR PACKAGER + CDN

### Database Tables Needed

```sql
-- Packager Monitoring
CREATE TABLE packager_metrics (
    time TIMESTAMP NOT NULL,
    probe_id INT,
    channel_id INT,
    
    -- Input (TS)
    ts_bitrate_kbps DECIMAL(10,2),
    ts_cc_errors INT,
    ts_packets_lost INT,
    
    -- Output (ABR)
    manifest_fetch_time_ms INT,
    segment_duration_sec DECIMAL(5,2),
    segment_count INT,
    segment_missing INT,
    
    -- ABR Ladder
    rung_count INT,
    rung_bitrates TEXT,  -- '500,2000,5000'
    
    PRIMARY KEY (time, probe_id, channel_id)
);

-- CDN Monitoring
CREATE TABLE cdn_metrics (
    time TIMESTAMP NOT NULL,
    probe_id INT,
    cdn_layer VARCHAR(20),  -- 'ORIGIN', 'MIDCACHE', 'EDGE'
    channel_id INT,
    
    -- HTTP Performance
    http_latency_ms INT,
    http_first_byte_ms INT,
    http_status_code INT,
    
    -- Cache
    cache_hit BOOLEAN,
    bytes_served INT,
    
    -- Errors
    error_count INT,
    timeout_count INT,
    
    PRIMARY KEY (time, probe_id, cdn_layer, channel_id)
);

-- Alert Log
CREATE TABLE alert_log (
    alert_id SERIAL PRIMARY KEY,
    time TIMESTAMP,
    severity VARCHAR(20),  -- CRITICAL, MAJOR, MINOR
    source VARCHAR(20),     -- PACKAGER, ORIGIN, MIDCACHE, EDGE
    channel_id INT,
    message TEXT,
    escalated_to VARCHAR(100),
    resolved_at TIMESTAMP
);
```

### Key Dashboards

#### Dashboard 1: Packager Health
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PACKAGER LAYER (L2) - LIVE MONITORING                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚ Status: â—â—â—â—â—  Packager-01 ONLINE                           â”‚
â”‚         â—â—â—â—   Packager-02 ONLINE                           â”‚
â”‚                                                              â”‚
â”‚ Channels Active: 50/50 âœ“                                    â”‚
â”‚ Manifests Generated: 50 âœ“                                   â”‚
â”‚ Average Segment Duration: 6.02s âœ“                          â”‚
â”‚                                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ TS Input (L1)       â”‚ ABR Output (to CDN)                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Total Bitrate:      â”‚ Rung Distribution:                 â”‚
â”‚ 2.5 Gbps (normal)   â”‚ â”œâ”€ 4K: 50 channels (100%)         â”‚
â”‚                     â”‚ â”œâ”€ HD: 50 channels (100%)         â”‚
â”‚ TS Errors:          â”‚ â”œâ”€ SD: 50 channels (100%)         â”‚
â”‚ 0/hour âœ“            â”‚ â””â”€ LD: 50 channels (100%)         â”‚
â”‚                     â”‚                                   â”‚
â”‚ Packet Loss:        â”‚ Segment Queue:                    â”‚
â”‚ 0.0% âœ“              â”‚ â”œâ”€ Pending: 150 segments         â”‚
â”‚                     â”‚ â”œâ”€ Ready: 5000 segments          â”‚
â”‚                     â”‚ â””â”€ Processing latency: 120ms     â”‚
â”‚                                                              â”‚
â”‚ Top Issues: [None]                                          â”‚
â”‚ Last Updated: 2025-01-13 15:45:30 UTC                      â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Dashboard 2: CDN Core Performance
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CDN CORE LAYER (L3) - ORIGIN + MIDCACHE PERFORMANCE         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚ ORIGIN HEALTH                                               â”‚
â”‚ â”œâ”€ Status: âœ“ ONLINE                                        â”‚
â”‚ â”œâ”€ HTTP Response Time (p95): 245ms (OK < 500ms)           â”‚
â”‚ â”œâ”€ HTTP Error Rate: 0.05% (OK < 1%)                       â”‚
â”‚ â”œâ”€ CPU Usage: 35% (OK < 80%)                              â”‚
â”‚ â”œâ”€ Memory: 18/32 GB (56%)                                 â”‚
â”‚ â””â”€ Disk: 450/500 GB (90%) - Acceptable                    â”‚
â”‚                                                              â”‚
â”‚ MIDCACHE PERFORMANCE                                        â”‚
â”‚ â”œâ”€ Region: Hanoi                                           â”‚
â”‚ â”œâ”€ Cache Hit Ratio: 92% (Target: > 90%) âœ“                 â”‚
â”‚ â”œâ”€ Cache Size: 420/500 GB                                 â”‚
â”‚ â”œâ”€ Avg Response Time: 120ms (fast)                        â”‚
â”‚ â”œâ”€ Eviction Rate: 150 objs/min (stable)                   â”‚
â”‚ â””â”€ Top Evicted: Long-tail channels (expected)            â”‚
â”‚                                                              â”‚
â”‚ PER-CHANNEL SAMPLE:                                        â”‚
â”‚ Channel   â”‚ Bitrate â”‚ Cache Hit â”‚ Origin Reqs/min          â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”‚
â”‚ CH_001    â”‚ 12.5Mbpsâ”‚ 98%       â”‚ 2 reqs/min (refill)     â”‚
â”‚ CH_002    â”‚ 11.2Mbpsâ”‚ 96%       â”‚ 3 reqs/min              â”‚
â”‚ CH_050    â”‚ 8.1Mbps â”‚ 87%       â”‚ 45 reqs/min (miss rate) â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¬ WEEK-BY-WEEK EXECUTION SUMMARY

```
WEEK 1: PACKAGER LAYER
  â”œâ”€ Day 1-2:   Infrastructure, Packager setup
  â”œâ”€ Day 2-3:   50 channel configuration
  â”œâ”€ Day 3-4:   Segment quality validation
  â””â”€ Day 5:     Test + freeze config

WEEK 2: CDN CORE LAYER
  â”œâ”€ Day 1-2:   CDN audit, Origin monitoring config
  â”œâ”€ Day 2-3:   MidCache performance tuning
  â”œâ”€ Day 3-4:   Cache hit ratio > 90%
  â”œâ”€ Day 4:     Edge POP readiness
  â””â”€ Day 5:     Test all failovers

WEEK 3: INTEGRATION + PRODUCTION
  â”œâ”€ Day 1-2:   End-to-end flow validation
  â”œâ”€ Day 2-3:   Threshold tuning, alert rules
  â”œâ”€ Day 3-4:   Team training, runbooks
  â”œâ”€ Day 4:     Production readiness review
  â””â”€ Day 5:     GO LIVE (Packager + CDN OTT revenue stream)

WEEK 4: STABILIZATION + L1 PLANNING
  â”œâ”€ Day 1-3:   24/7 monitoring, incident response
  â”œâ”€ Day 3-4:   Baseline data collection
  â”œâ”€ Day 4-5:   Plan L1 (Headend) deployment next
  â””â”€ Review:    What went well? What to improve?
```

---

## ğŸ“‹ DELIVERABLES CHECKLIST (PACKAGER + CDN)

```
WEEK 1 (Packager):
  â˜ Packager infra verified (CPU, disk, network)
  â˜ 50 channels configured in packager
  â˜ All channels generating HLS/DASH manifests
  â˜ Segment quality checker script deployed
  â˜ Segment quality dashboard live (Grafana)
  â˜ Inspector LIVE #2 (Packager probe) receiving all 50 channels
  â˜ Redundancy (Packager-01 + Packager-02) configured
  â˜ Failover test: Packager-01 down â†’ Packager-02 serves âœ“

WEEK 2 (CDN):
  â˜ CDN topology documented
  â˜ Origin monitoring live (Inspector LIVE #3)
  â˜ Origin latency baseline established
  â˜ MidCache cache hit ratio > 90% achieved
  â˜ Per-channel cache performance documented
  â˜ Edge POPs pre-warmed with top 20 channels
  â˜ Edge POP failover tested (origin down â†’ edge cache serves)
  â˜ CDN health dashboard live (Grafana)

WEEK 3 (Integration):
  â˜ End-to-end TS â†’ Packager â†’ CDN â†’ Edge validated
  â˜ MOS metric tracked from Packager + CDN layers
  â˜ Alert routing tested: Packager issue â†’ Packager team
  â˜ Alert routing tested: CDN issue â†’ CDN team
  â˜ Thresholds baselined on 24h+ data
  â˜ Runbooks written for Packager team
  â˜ Runbooks written for CDN team
  â˜ Training completed (Packager ops + CDN ops)
  â˜ Redundancy (active-active, active-passive) verified
  â˜ Production readiness sign-off obtained

WEEK 4+ (Operations):
  â˜ Monitoring stable, false alert rate < 5%
  â˜ MTTR (mean time to resolution) < 15 min for issues
  â˜ Weekly review meeting scheduled
  â˜ Incident post-mortems completed
  â˜ Next phase (L1 Headend) planning in progress
```

---

## ğŸš¨ PACKAGER + CDN CRITICAL SUCCESS FACTORS

```
1. SEGMENT CONTINUITY
   â””â”€ No gaps, no duplicates, predictable timing
   â””â”€ If fails â†’ playback stalls, rebuffers
   â””â”€ MONITOR: Sequence number jumps, duration variance

2. CACHE HIT RATIO > 90%
   â””â”€ Reduces origin load, improves edge latency
   â””â”€ If drops < 85% â†’ origin can be overwhelmed
   â””â”€ MONITOR: HIT vs MISS ratio hourly

3. HTTP ERROR RATE < 0.1%
   â””â”€ 404s, 5xx errors mean lost segments
   â””â”€ Users see black screen or buffering
   â””â”€ MONITOR: Every HTTP error status code

4. LATENCY P95 < 500ms (Origin)
   â””â”€ Segment download must complete before buffer empty
   â””â”€ If latency > 1s â†’ client stalls
   â””â”€ MONITOR: Response time percentiles (p50, p95, p99)

5. ABR LADDER INTACT
   â””â”€ All rungs available (4K, HD, SD, LD)
   â””â”€ No "holes" in bitrate ladder
   â””â”€ If 4K missing â†’ can't serve 4K users
   â””â”€ MONITOR: Manifest validation, rendition presence

6. REDUNDANCY WORKING
   â””â”€ Packager failover < 5 sec
   â””â”€ Origin failover < 10 sec
   â””â”€ If failover doesn't work â†’ outage possible
   â””â”€ MONITOR: Failover testing monthly

7. MANIFEST CONSISTENCY
   â””â”€ Master + variant playlists in sync
   â””â”€ Segment count, duration, bitrate consistent
   â””â”€ If inconsistent â†’ player confusion
   â””â”€ MONITOR: Playlist validation rules
```

---

## ğŸ“ TEAM CONTACTS + ESCALATION

```
PACKAGER ISSUES:
â”œâ”€ Primary: Packager Engineer
â”‚   Name: [Name]
â”‚   Email: packager-team@fpt.com.vn
â”‚   Phone: +84-xxx-yyy-zzzz
â”‚   On-call: Yes (24/7)
â”‚
â””â”€ Escalation: Packager Lead
    Name: [Lead Name]
    Email: [Lead Email]
    Phone: [Lead Phone]

CDN ISSUES:
â”œâ”€ Primary: CDN Engineer
â”‚   Name: [Name]
â”‚   Email: cdn-team@fpt.com.vn
â”‚   Phone: +84-xxx-yyy-zzzz
â”‚   On-call: Yes (24/7)
â”‚
â””â”€ Escalation: CDN Lead
    Name: [Lead Name]
    Email: [Lead Email]
    Phone: [Lead Phone]

NOC (Monitoring Center):
â”œâ”€ Shift 1 (08:00-16:00): [NOC Ops 1]
â”œâ”€ Shift 2 (16:00-00:00): [NOC Ops 2]
â”œâ”€ Shift 3 (00:00-08:00): [NOC Ops 3]
â””â”€ Emergency: +84-xxx-yyy-zzzz (24/7 hotline)

MANAGEMENT:
â”œâ”€ Platform Lead: [Name] - platform-lead@fpt.com.vn
â””â”€ Ops Director: [Name] - ops-director@fpt.com.vn
```

---

## âœ… GO-LIVE READINESS (END OF WEEK 3)

Before go-live, VERIFY:

```
â˜ All 50 channels streaming OTT via Packager + CDN
â˜ No client reports of buffering or quality issues
â˜ Baseline MOS on ABR tracks TS MOS
â˜ Alert response time < 5 min for Critical alerts
â˜ Failover tests successful (no user impact)
â˜ Team trained and on-call ready
â˜ Documentation complete and accessible
â˜ Incident playbooks reviewed
â˜ Escalation list updated
â˜ Management sign-off obtained
â˜ Customer (business stakeholders) notified

DATE GO-LIVE AUTHORIZED: _______________
SIGNED BY: ___________________________
```

---

**END - Packager + CDN Deployment First Strategy**

ğŸ¯ **NEXT PHASE**: After Packager + CDN stable (Week 4+)
   â†’ Deploy L1 (Headend Encoder Probe)
   â†’ Then L4 (Edge POP Probes)
   â†’ Finally L0 (Contribution monitoring)

